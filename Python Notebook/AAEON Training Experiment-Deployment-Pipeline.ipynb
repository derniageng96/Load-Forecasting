{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"},"colab":{"name":"AAEON Training Experiment-Deployment-Pipeline.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jm-sCYLsaGop"},"source":["# Import Libray"]},{"cell_type":"code","metadata":{"id":"NoLLC3T4QDwc","executionInfo":{"status":"ok","timestamp":1624964749414,"user_tz":-480,"elapsed":4061,"user":{"displayName":"Derni Ageng","photoUrl":"","userId":"01065409850554572009"}}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import plotly.graph_objects as go\n","import plotly.offline as py\n","import matplotlib.pyplot as plt\n","import math\n","import pickle\n","from keras.callbacks import EarlyStopping\n","from numpy import concatenate"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1piGrJvaJr7"},"source":["# Define function \n","## Helper consist of 2 function \n","\n","### 1. Mean Absolute Error \n","To calculate loss from the models\n","\n","### 2. generateDataset\n","\n","To form a time series set \n","\n","## train_LSTM_Model\n","\n","Construct the training and testing dataset and deep learning framework\n"]},{"cell_type":"code","metadata":{"id":"9mbPd0RCQDwe","executionInfo":{"status":"ok","timestamp":1624964827841,"user_tz":-480,"elapsed":453,"user":{"displayName":"Derni Ageng","photoUrl":"","userId":"01065409850554572009"}}},"source":["class Helper:\n","    def mean_absolute_percentage_error(self,y_true, y_pred):\n","        y_true, y_pred = np.array(y_true), np.array(y_pred)\n","        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","    def generateDataSet(self, data, n_in=1, n_out=1, dropnan=True):\n","        n_vars = 1 if type(data) is list else data.shape[1]\n","        df = pd.DataFrame(data)\n","        cols, names = list(), list()\n","        # input sequence (t-n, ... t-1)\n","        for i in range(n_in, 0, -1):\n","            cols.append(df.shift(i))\n","            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n","        # forecast sequence (t, t+1, ... t+n)\n","        for i in range(0, n_out):\n","            cols.append(df.shift(-i))\n","            if i == 0:\n","                names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n","            else:\n","                names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n","        # put it all together\n","        agg = pd.concat(cols, axis=1)\n","        agg.columns = names\n","        # drop rows with NaN values\n","        if dropnan:\n","            agg.dropna(inplace=True)\n","        return agg\n","\n","    def train_LSTM_model(self,reframed):\n","        n_hours = 9\n","        n_features = 5\n","        values = reframed.values\n","        # Define number of training and testing set # 60 for two months 150 for 5 months\n","        n_train_hours = 24*150 \n","        train = values[:n_train_hours, :]\n","        test = values[n_train_hours:, :]\n","        # split into input and outputs\n","        n_obs = n_hours*n_features\n","        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n","        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n","        print(train_X.shape, len(train_X), train_y.shape)\n","        # reshape input to be 3D [samples, timesteps, features]\n","        train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n","        test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n","        print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n","\n","        # design network\n","        model = keras.Sequential()\n","        model.add(layers.LSTM(50, activation='linear',input_shape=(train_X.shape[1], train_X.shape[2])))\n","        model.add(layers.Dense(50,activation='linear'))\n","        model.add(layers.Dense(1))\n","        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=50)\n","        model.compile(loss='mean_squared_error', optimizer='adam')\n","        # fit network\n","        history = model.fit(train_X, train_y, epochs=1000, batch_size=24, validation_data=(test_X[:1800], test_y[:1800]), verbose=2, shuffle=False, callbacks=[es])\n","\n","        yhat = model.predict(test_X)\n","        test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n","        inv_yhat = concatenate((yhat, test_X[:, -4:]), axis=1)                        \n","        inv_yhat = inv_yhat[:,0]\n","        test_y = test_y.reshape((len(test_y), 1))\n","        inv_y = concatenate((test_y, test_X[:, -4:]), axis=1)\n","        inv_y = inv_y[:,0]\n","        # calculate RMSE of inversed value \n","        rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n","        print('Test RMSE: %.3f' % rmse) \n","\n","        pct = self.mean_absolute_percentage_error(inv_y,inv_yhat)\n","        print('MAPE : %.3f' % pct +'%' )\n","\n","        return model"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOdzEryBa1dX"},"source":["# Load Dataset and use the function for load forecasting"]},{"cell_type":"code","metadata":{"id":"nvTelLKGQDwg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624964939016,"user_tz":-480,"elapsed":99549,"user":{"displayName":"Derni Ageng","photoUrl":"","userId":"01065409850554572009"}},"outputId":"560f825c-82ef-497c-f860-8dc75cc67d1f"},"source":["def main():\n","    pw=pd.read_csv('changyuanbuilding_interpolation_SGF_202007-202104.csv')\n","\n","    dataset = pd.DataFrame()\n","    dataset = pw[['p_sum','ae_tot','weekend','Temperature','session']]\n","    # dataset = pw[['p_sum','ae_tot','weekend','Temp(Â¢J)','session']]\n","    print(dataset.head())\n","\n","    helper = Helper()\n","    reframed = helper.generateDataSet(dataset, 9, 1)\n","    model = helper.train_LSTM_model(reframed)\n","    model.save(\"model\")\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["         p_sum        ae_tot  weekend  Temperature  session\n","0  9308.932296  27153.204735        0         27.2        1\n","1  9246.681200  27161.810227        0         29.1        0\n","2  9184.430104  27171.934375        0         28.9        0\n","3  9122.179008  27182.831723        0         28.6        0\n","4  9059.927912  27191.283333        0         28.5        0\n","(3600, 45) 3600 (3600,)\n","(3600, 9, 5) (3600,) (3639, 9, 5) (3639,)\n","Epoch 1/1000\n","150/150 - 2s - loss: 5561590.5000 - val_loss: 1370832.2500\n","Epoch 2/1000\n","150/150 - 1s - loss: 3183979.7500 - val_loss: 925804.8750\n","Epoch 3/1000\n","150/150 - 1s - loss: 2398315.2500 - val_loss: 532039.5000\n","Epoch 4/1000\n","150/150 - 1s - loss: 1804754.3750 - val_loss: 632010.1250\n","Epoch 5/1000\n","150/150 - 1s - loss: 1479772.2500 - val_loss: 680757.6250\n","Epoch 6/1000\n","150/150 - 1s - loss: 2772097.2500 - val_loss: 325416.9688\n","Epoch 7/1000\n","150/150 - 1s - loss: 2015535.8750 - val_loss: 345763.3438\n","Epoch 8/1000\n","150/150 - 1s - loss: 1275098.1250 - val_loss: 417093.6562\n","Epoch 9/1000\n","150/150 - 1s - loss: 852689.6250 - val_loss: 196269.0781\n","Epoch 10/1000\n","150/150 - 1s - loss: 858435.9375 - val_loss: 181421.5000\n","Epoch 11/1000\n","150/150 - 1s - loss: 600037.8750 - val_loss: 137468.9219\n","Epoch 12/1000\n","150/150 - 1s - loss: 394009.1250 - val_loss: 120078.9062\n","Epoch 13/1000\n","150/150 - 1s - loss: 270601.0625 - val_loss: 69113.7266\n","Epoch 14/1000\n","150/150 - 1s - loss: 361474.0000 - val_loss: 74570.4453\n","Epoch 15/1000\n","150/150 - 1s - loss: 238679.9688 - val_loss: 52628.9414\n","Epoch 16/1000\n","150/150 - 1s - loss: 251041.2969 - val_loss: 204478.1719\n","Epoch 17/1000\n","150/150 - 1s - loss: 240701.5156 - val_loss: 147876.1719\n","Epoch 18/1000\n","150/150 - 1s - loss: 221959.2031 - val_loss: 219043.2500\n","Epoch 19/1000\n","150/150 - 1s - loss: 190239.3125 - val_loss: 205236.4062\n","Epoch 20/1000\n","150/150 - 1s - loss: 200092.2656 - val_loss: 148107.1719\n","Epoch 21/1000\n","150/150 - 1s - loss: 219125.6562 - val_loss: 59439.7383\n","Epoch 22/1000\n","150/150 - 1s - loss: 176734.3125 - val_loss: 84338.1953\n","Epoch 23/1000\n","150/150 - 1s - loss: 181408.9219 - val_loss: 62495.6328\n","Epoch 24/1000\n","150/150 - 1s - loss: 166459.7031 - val_loss: 58177.8320\n","Epoch 25/1000\n","150/150 - 1s - loss: 166439.7656 - val_loss: 61293.1328\n","Epoch 26/1000\n","150/150 - 1s - loss: 167555.7656 - val_loss: 214530.7969\n","Epoch 27/1000\n","150/150 - 1s - loss: 155106.5781 - val_loss: 168945.4375\n","Epoch 28/1000\n","150/150 - 1s - loss: 148011.0469 - val_loss: 157807.2344\n","Epoch 29/1000\n","150/150 - 1s - loss: 137477.2344 - val_loss: 149941.2969\n","Epoch 30/1000\n","150/150 - 1s - loss: 136676.4688 - val_loss: 89210.6562\n","Epoch 31/1000\n","150/150 - 1s - loss: 130515.7031 - val_loss: 77675.4766\n","Epoch 32/1000\n","150/150 - 1s - loss: 127830.3594 - val_loss: 80642.1797\n","Epoch 33/1000\n","150/150 - 1s - loss: 123422.6406 - val_loss: 68697.0078\n","Epoch 34/1000\n","150/150 - 1s - loss: 118403.8203 - val_loss: 85387.3828\n","Epoch 35/1000\n","150/150 - 1s - loss: 114007.4375 - val_loss: 73872.9453\n","Epoch 36/1000\n","150/150 - 1s - loss: 110863.2109 - val_loss: 68180.9688\n","Epoch 37/1000\n","150/150 - 1s - loss: 107557.3828 - val_loss: 77426.0469\n","Epoch 38/1000\n","150/150 - 1s - loss: 101195.0703 - val_loss: 88810.1016\n","Epoch 39/1000\n","150/150 - 1s - loss: 98922.9844 - val_loss: 87926.1406\n","Epoch 40/1000\n","150/150 - 1s - loss: 94473.3359 - val_loss: 84166.3359\n","Epoch 41/1000\n","150/150 - 1s - loss: 90925.0547 - val_loss: 79654.1953\n","Epoch 42/1000\n","150/150 - 1s - loss: 87575.2109 - val_loss: 90076.2812\n","Epoch 43/1000\n","150/150 - 1s - loss: 86214.3359 - val_loss: 60206.7656\n","Epoch 44/1000\n","150/150 - 1s - loss: 82463.6250 - val_loss: 45724.7070\n","Epoch 45/1000\n","150/150 - 1s - loss: 80377.4375 - val_loss: 32148.6582\n","Epoch 46/1000\n","150/150 - 1s - loss: 76476.6406 - val_loss: 36004.7344\n","Epoch 47/1000\n","150/150 - 1s - loss: 74022.0703 - val_loss: 34734.3086\n","Epoch 48/1000\n","150/150 - 1s - loss: 218704.6094 - val_loss: 115115.7812\n","Epoch 49/1000\n","150/150 - 1s - loss: 75921.8047 - val_loss: 96667.0312\n","Epoch 50/1000\n","150/150 - 1s - loss: 78923.5547 - val_loss: 71510.0469\n","Epoch 51/1000\n","150/150 - 1s - loss: 70049.8047 - val_loss: 61436.8477\n","Epoch 52/1000\n","150/150 - 1s - loss: 66596.5000 - val_loss: 59195.2266\n","Epoch 53/1000\n","150/150 - 1s - loss: 64872.8086 - val_loss: 62780.0547\n","Epoch 54/1000\n","150/150 - 1s - loss: 63272.3555 - val_loss: 46143.3281\n","Epoch 55/1000\n","150/150 - 1s - loss: 60352.3008 - val_loss: 35761.8242\n","Epoch 56/1000\n","150/150 - 1s - loss: 58853.8789 - val_loss: 39106.9062\n","Epoch 57/1000\n","150/150 - 1s - loss: 60106.8438 - val_loss: 64606.7305\n","Epoch 58/1000\n","150/150 - 1s - loss: 56996.7773 - val_loss: 49163.4141\n","Epoch 59/1000\n","150/150 - 1s - loss: 54563.4766 - val_loss: 41738.0938\n","Epoch 60/1000\n","150/150 - 1s - loss: 51282.3477 - val_loss: 27640.8984\n","Epoch 61/1000\n","150/150 - 1s - loss: 48080.0703 - val_loss: 16465.5293\n","Epoch 62/1000\n","150/150 - 1s - loss: 48759.4648 - val_loss: 17532.3984\n","Epoch 63/1000\n","150/150 - 1s - loss: 46458.9297 - val_loss: 16218.6309\n","Epoch 64/1000\n","150/150 - 1s - loss: 43064.7461 - val_loss: 18127.6934\n","Epoch 65/1000\n","150/150 - 1s - loss: 46842.5742 - val_loss: 35794.8047\n","Epoch 66/1000\n","150/150 - 1s - loss: 46212.7617 - val_loss: 35104.0547\n","Epoch 67/1000\n","150/150 - 1s - loss: 46150.3555 - val_loss: 29762.2793\n","Epoch 68/1000\n","150/150 - 1s - loss: 41598.4570 - val_loss: 30340.4785\n","Epoch 69/1000\n","150/150 - 1s - loss: 38857.5391 - val_loss: 29065.7070\n","Epoch 70/1000\n","150/150 - 1s - loss: 37227.8320 - val_loss: 26136.6133\n","Epoch 71/1000\n","150/150 - 1s - loss: 36036.9102 - val_loss: 24136.7617\n","Epoch 72/1000\n","150/150 - 1s - loss: 35021.1406 - val_loss: 22575.2070\n","Epoch 73/1000\n","150/150 - 1s - loss: 35008.6523 - val_loss: 24692.1953\n","Epoch 74/1000\n","150/150 - 1s - loss: 33891.5117 - val_loss: 22626.4414\n","Epoch 75/1000\n","150/150 - 1s - loss: 33227.9180 - val_loss: 22063.0938\n","Epoch 76/1000\n","150/150 - 1s - loss: 31926.8984 - val_loss: 17247.5195\n","Epoch 77/1000\n","150/150 - 1s - loss: 30738.2148 - val_loss: 17183.0898\n","Epoch 78/1000\n","150/150 - 1s - loss: 29780.8105 - val_loss: 16827.6953\n","Epoch 79/1000\n","150/150 - 1s - loss: 29606.0059 - val_loss: 16132.5938\n","Epoch 80/1000\n","150/150 - 1s - loss: 28945.3867 - val_loss: 15628.0557\n","Epoch 81/1000\n","150/150 - 1s - loss: 28216.1777 - val_loss: 14933.3281\n","Epoch 82/1000\n","150/150 - 1s - loss: 28010.8711 - val_loss: 14711.4756\n","Epoch 83/1000\n","150/150 - 1s - loss: 27834.9180 - val_loss: 14229.7275\n","Epoch 84/1000\n","150/150 - 1s - loss: 27581.1934 - val_loss: 14617.1982\n","Epoch 85/1000\n","150/150 - 1s - loss: 28074.2480 - val_loss: 14190.0957\n","Epoch 86/1000\n","150/150 - 1s - loss: 26957.0215 - val_loss: 14461.6631\n","Epoch 87/1000\n","150/150 - 1s - loss: 27501.6641 - val_loss: 15238.3525\n","Epoch 88/1000\n","150/150 - 1s - loss: 28575.8828 - val_loss: 16061.1230\n","Epoch 89/1000\n","150/150 - 1s - loss: 724861.8750 - val_loss: 54426.4922\n","Epoch 90/1000\n","150/150 - 1s - loss: 45266.6055 - val_loss: 54097.6836\n","Epoch 91/1000\n","150/150 - 1s - loss: 48345.8828 - val_loss: 56848.2656\n","Epoch 92/1000\n","150/150 - 1s - loss: 44303.2031 - val_loss: 45906.6758\n","Epoch 93/1000\n","150/150 - 1s - loss: 43114.3203 - val_loss: 46159.4180\n","Epoch 94/1000\n","150/150 - 1s - loss: 40866.8359 - val_loss: 22303.0000\n","Epoch 95/1000\n","150/150 - 1s - loss: 42024.4180 - val_loss: 15559.7412\n","Epoch 96/1000\n","150/150 - 1s - loss: 41774.6914 - val_loss: 16234.7207\n","Epoch 97/1000\n","150/150 - 1s - loss: 39205.4492 - val_loss: 15684.9854\n","Epoch 98/1000\n","150/150 - 1s - loss: 40294.7070 - val_loss: 16994.3184\n","Epoch 99/1000\n","150/150 - 1s - loss: 48826.8164 - val_loss: 13827.5186\n","Epoch 100/1000\n","150/150 - 1s - loss: 43433.2227 - val_loss: 18074.5020\n","Epoch 101/1000\n","150/150 - 1s - loss: 40123.5898 - val_loss: 16542.4199\n","Epoch 102/1000\n","150/150 - 1s - loss: 38156.1406 - val_loss: 17801.6113\n","Epoch 103/1000\n","150/150 - 1s - loss: 37434.1562 - val_loss: 17322.2461\n","Epoch 104/1000\n","150/150 - 1s - loss: 34396.0156 - val_loss: 13298.3877\n","Epoch 105/1000\n","150/150 - 1s - loss: 34363.7812 - val_loss: 13074.9297\n","Epoch 106/1000\n","150/150 - 1s - loss: 35495.3477 - val_loss: 13350.9775\n","Epoch 107/1000\n","150/150 - 1s - loss: 34396.7383 - val_loss: 13001.4229\n","Epoch 108/1000\n","150/150 - 1s - loss: 37160.6289 - val_loss: 13307.6777\n","Epoch 109/1000\n","150/150 - 1s - loss: 93379.5547 - val_loss: 31513.6309\n","Epoch 110/1000\n","150/150 - 1s - loss: 35851.0664 - val_loss: 31812.1934\n","Epoch 111/1000\n","150/150 - 1s - loss: 30677.4336 - val_loss: 29968.7324\n","Epoch 112/1000\n","150/150 - 1s - loss: 31084.0898 - val_loss: 25964.8652\n","Epoch 113/1000\n","150/150 - 1s - loss: 31431.1426 - val_loss: 21889.0859\n","Epoch 114/1000\n","150/150 - 1s - loss: 31717.3594 - val_loss: 19200.0977\n","Epoch 115/1000\n","150/150 - 1s - loss: 31823.5820 - val_loss: 17297.9785\n","Epoch 116/1000\n","150/150 - 1s - loss: 31453.9062 - val_loss: 15554.0771\n","Epoch 117/1000\n","150/150 - 1s - loss: 30915.6836 - val_loss: 14621.4893\n","Epoch 118/1000\n","150/150 - 1s - loss: 30917.2617 - val_loss: 13902.6162\n","Epoch 119/1000\n","150/150 - 1s - loss: 31236.8516 - val_loss: 13280.3271\n","Epoch 120/1000\n","150/150 - 1s - loss: 31656.9883 - val_loss: 13636.6826\n","Epoch 121/1000\n","150/150 - 1s - loss: 31026.9043 - val_loss: 14167.9092\n","Epoch 122/1000\n","150/150 - 1s - loss: 30169.8867 - val_loss: 13486.9629\n","Epoch 123/1000\n","150/150 - 1s - loss: 32758.3281 - val_loss: 15957.5479\n","Epoch 124/1000\n","150/150 - 1s - loss: 31251.4473 - val_loss: 19353.8555\n","Epoch 125/1000\n","150/150 - 1s - loss: 29864.0977 - val_loss: 18311.9258\n","Epoch 126/1000\n","150/150 - 1s - loss: 29625.5977 - val_loss: 19323.3066\n","Epoch 127/1000\n","150/150 - 1s - loss: 29290.3379 - val_loss: 20249.1504\n","Epoch 128/1000\n","150/150 - 1s - loss: 29115.9512 - val_loss: 21145.6113\n","Epoch 129/1000\n","150/150 - 1s - loss: 29445.4004 - val_loss: 23306.2363\n","Epoch 130/1000\n","150/150 - 1s - loss: 29081.8887 - val_loss: 23225.9805\n","Epoch 131/1000\n","150/150 - 1s - loss: 28539.6387 - val_loss: 22655.4668\n","Epoch 132/1000\n","150/150 - 1s - loss: 27940.0332 - val_loss: 22701.8203\n","Epoch 133/1000\n","150/150 - 1s - loss: 27890.3242 - val_loss: 22801.6016\n","Epoch 134/1000\n","150/150 - 1s - loss: 27343.1543 - val_loss: 19358.3848\n","Epoch 135/1000\n","150/150 - 1s - loss: 26992.6914 - val_loss: 20172.1289\n","Epoch 136/1000\n","150/150 - 1s - loss: 26579.3691 - val_loss: 19605.3887\n","Epoch 137/1000\n","150/150 - 1s - loss: 26601.9824 - val_loss: 19545.4180\n","Epoch 138/1000\n","150/150 - 1s - loss: 26659.3398 - val_loss: 19675.4141\n","Epoch 139/1000\n","150/150 - 1s - loss: 26347.0098 - val_loss: 19587.2520\n","Epoch 140/1000\n","150/150 - 1s - loss: 26209.3535 - val_loss: 19337.0391\n","Epoch 141/1000\n","150/150 - 1s - loss: 25647.8672 - val_loss: 17680.1387\n","Epoch 142/1000\n","150/150 - 1s - loss: 25349.4258 - val_loss: 16777.0176\n","Epoch 143/1000\n","150/150 - 1s - loss: 25488.6172 - val_loss: 15042.3174\n","Epoch 144/1000\n","150/150 - 1s - loss: 25474.3574 - val_loss: 17309.8281\n","Epoch 145/1000\n","150/150 - 1s - loss: 24663.4961 - val_loss: 16496.5312\n","Epoch 146/1000\n","150/150 - 1s - loss: 24582.6875 - val_loss: 15913.0273\n","Epoch 147/1000\n","150/150 - 1s - loss: 24684.4453 - val_loss: 16279.7168\n","Epoch 148/1000\n","150/150 - 1s - loss: 24608.7910 - val_loss: 16630.9941\n","Epoch 149/1000\n","150/150 - 1s - loss: 24055.7637 - val_loss: 16412.3672\n","Epoch 150/1000\n","150/150 - 1s - loss: 24266.4707 - val_loss: 15704.0176\n","Epoch 151/1000\n","150/150 - 1s - loss: 23736.5332 - val_loss: 16332.3789\n","Epoch 152/1000\n","150/150 - 1s - loss: 23690.5586 - val_loss: 16450.7441\n","Epoch 153/1000\n","150/150 - 1s - loss: 23515.7754 - val_loss: 16148.1748\n","Epoch 154/1000\n","150/150 - 1s - loss: 23364.1016 - val_loss: 16187.1064\n","Epoch 155/1000\n","150/150 - 1s - loss: 23308.2461 - val_loss: 16207.7812\n","Epoch 156/1000\n","150/150 - 1s - loss: 23330.3691 - val_loss: 16630.1738\n","Epoch 157/1000\n","150/150 - 1s - loss: 23190.7324 - val_loss: 16822.5371\n","Epoch 00157: early stopping\n","Test RMSE: 122.220\n","MAPE : 8.892%\n","INFO:tensorflow:Assets written to: model/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8Przy894g9UP"},"source":["Load model"]},{"cell_type":"code","metadata":{"id":"3HtIxKuzhSXL"},"source":["model = tf.keras.models.load_model('model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9z7kCQUhaMB","executionInfo":{"status":"ok","timestamp":1624798385037,"user_tz":-480,"elapsed":270,"user":{"displayName":"Derni Ageng","photoUrl":"","userId":"01065409850554572009"}},"outputId":"d698910a-e711-4b8c-9121-9b8c61f3c433"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.engine.sequential.Sequential at 0x7f5994e9cb90>"]},"metadata":{"tags":[]},"execution_count":7}]}]}